{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "files = ['legit','phish']\n",
    "\n",
    "# Create an empty list to store the dictionaries\n",
    "dict_list = []\n",
    "    \n",
    "for file in files:\n",
    "    \n",
    "    with open(f'{file}_preprocessed_json.json') as fp:\n",
    "        data = json.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "    # Iterate over each item in the JSON data\n",
    "    for item in data:\n",
    "        if item != {}:\n",
    "            if \"1\" not in item[\"rejected-for\"]:\n",
    "                # Extract the subject and header values\n",
    "                subject = item['header']['Subject']\n",
    "                body = item['body']\n",
    "\n",
    "                # Create a dictionary with subject and header\n",
    "                dict_item = {'text': subject+\" \"+body, 'labels': file}\n",
    "                \n",
    "                # Append the dictionary to the list\n",
    "                dict_list.append(dict_item)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legit    3730\n",
       "phish     496\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_prabowo_ml import clean_html, remove_links, non_ascii, lower, email_address, removeStopWords, punct, remove_\n",
    "import re\n",
    "\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# PREPROCESS THE DATA\n",
    "def preproc(df, colname):\n",
    "  df[colname] = df[colname].apply(func=clean_html)\n",
    "  df[colname] = df[colname].apply(func=remove_links)\n",
    "  df[colname] = df[colname].apply(func=non_ascii)\n",
    "  df[colname] = df[colname].apply(func=lower)\n",
    "  df[colname] = df[colname].apply(func=email_address)\n",
    "  # df[colname] = df[colname].apply(func=removeStopWords)\n",
    "  df[colname] = df[colname].apply(func=punct)\n",
    "  df[colname] = df[colname].apply(func=remove_)\n",
    "  return(df)\n",
    "\n",
    "df_clean = preproc(df, 'text')\n",
    "df_clean.drop('index', axis=1, inplace=True)\n",
    "df_clean['num_words'] = df_clean['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Convert 'labels' column to categorical data type\n",
    "df_clean['labels'] = pd.Categorical(df_clean['labels'])\n",
    "\n",
    "df_clean['labels'] = df_clean['labels'].cat.codes\n",
    "encoded_dict = {'legit':0, 'phish':1} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592df76e6b4043eebd9ed9334f902cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VENUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VENUS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFBertModel \n",
    "\n",
    "df_train, df_test = train_test_split(df_clean, test_size=0.3, random_state=42,\n",
    "                                     stratify=df_clean['labels'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "max_len = 70\n",
    "\n",
    "X_train = tokenizer(\n",
    "    text=df_train['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=df_test['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "93/93 [==============================] - 1046s 11s/step - loss: 0.1095 - binary_accuracy: 0.9628 - val_loss: 0.0675 - val_binary_accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "# embeddings = dbert_model(input_ids, attention_mask = input_mask)[0]\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask = input_mask)[0] # 0 = last hidden state, 1 = poller_output\n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32, activation='relu')(out)\n",
    "\n",
    "y = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=[y])\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "loss = BinaryCrossentropy()\n",
    "metric = BinaryAccuracy()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metric\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x = {'input_ids':X_train['input_ids'], 'attention_mask':X_train['attention_mask']},\n",
    "    y = df_train['labels'],\n",
    "    validation_data = ({\n",
    "        'input_ids':X_test['input_ids'], 'attention_mask':X_test['attention_mask']},\n",
    "                       df_test['labels']),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 123s 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      1119\n",
      "           1       0.00      0.00      0.00       149\n",
      "\n",
      "    accuracy                           0.88      1268\n",
      "   macro avg       0.44      0.50      0.47      1268\n",
      "weighted avg       0.78      0.88      0.83      1268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']})\n",
    "y_predicted = np.argmax(predicted, axis=1)\n",
    "print(classification_report(df_test['labels'], y_predicted,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1119\n",
       "1     149\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>re i may have a meeting around 3pm i have to g...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r re r re r re r fw tax form dear mesfer thank...</td>\n",
       "      <td>0</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thank you just got your get well package thank...</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r re r re r re r fw tax form dear mesfer thank...</td>\n",
       "      <td>0</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for rr2 trump invests in the same companies he...</td>\n",
       "      <td>0</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>scotiabank important update dear customer scot...</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>unauthorized access to your paypal account bod...</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>paypal please restore your account dear paypal...</td>\n",
       "      <td>1</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>billing issues dear valued ebay member we rece...</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>paypal account security measures dear paypal m...</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4226 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  num_words\n",
       "0     re i may have a meeting around 3pm i have to g...       0         25\n",
       "1     r re r re r re r fw tax form dear mesfer thank...       0       1961\n",
       "2     thank you just got your get well package thank...       0         32\n",
       "3     r re r re r re r fw tax form dear mesfer thank...       0       1961\n",
       "4     for rr2 trump invests in the same companies he...       0        776\n",
       "...                                                 ...     ...        ...\n",
       "4221  scotiabank important update dear customer scot...       1        243\n",
       "4222  unauthorized access to your paypal account bod...       1        248\n",
       "4223  paypal please restore your account dear paypal...       1        310\n",
       "4224  billing issues dear valued ebay member we rece...       1        203\n",
       "4225  paypal account security measures dear paypal m...       1        117\n",
       "\n",
       "[4226 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
