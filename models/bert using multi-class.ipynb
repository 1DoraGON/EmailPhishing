{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote:\\n\\nRejected-for:\\n0 - Not Rejected (default value)\\n1 - Missing Subject or Body\\n2 - Unacceptable Header Size\\n3 - Unacceptable Body Size\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "files = ['phish','legit']\n",
    "\n",
    "# Create an empty list to store the dictionaries\n",
    "dict_list = []\n",
    "    \n",
    "for file in files:\n",
    "    \n",
    "    with open(f'{file}_preprocessed_json.json') as fp:\n",
    "        data = json.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "    # Iterate over each item in the JSON data\n",
    "    for item in data:\n",
    "        if item != {}:\n",
    "            if \"1\" not in item[\"rejected-for\"]:\n",
    "                # Extract the subject and header values\n",
    "                subject = item['header']['Subject']\n",
    "                body = item['body']\n",
    "\n",
    "                # Create a dictionary with subject and header\n",
    "                dict_item = {'text': subject+\" \"+body, 'labels': file}\n",
    "                \n",
    "                # Append the dictionary to the list\n",
    "                dict_list.append(dict_item)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(dict_list)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "\n",
    "Rejected-for:\n",
    "0 - Not Rejected (default value)\n",
    "1 - Missing Subject or Body\n",
    "2 - Unacceptable Header Size\n",
    "3 - Unacceptable Body Size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legit    3730\n",
       "phish     496\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paypal flagged account dear paypal member, you...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uh security alert uh security alert=2c =a0a dg...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>account review department online banking profi...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re:websit hi there ! web site ... on -line- my...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you have one new message at capital one. dear ...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>democrats hold trump accountable across the co...</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>daily political guidance -- wednesday, april 2...</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>re: organizationchair next tuesday? will get r...</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>re: dws arizona republic op-ed is now up i'll ...</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>cruz email: our new ad (with carly!!) sent fro...</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4226 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text labels\n",
       "0     paypal flagged account dear paypal member, you...  phish\n",
       "1     uh security alert uh security alert=2c =a0a dg...  phish\n",
       "2     account review department online banking profi...  phish\n",
       "3     re:websit hi there ! web site ... on -line- my...  phish\n",
       "4     you have one new message at capital one. dear ...  phish\n",
       "...                                                 ...    ...\n",
       "4221  democrats hold trump accountable across the co...  legit\n",
       "4222  daily political guidance -- wednesday, april 2...  legit\n",
       "4223  re: organizationchair next tuesday? will get r...  legit\n",
       "4224  re: dws arizona republic op-ed is now up i'll ...  legit\n",
       "4225  cruz email: our new ad (with carly!!) sent fro...  legit\n",
       "\n",
       "[4226 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VENUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_prabowo_ml import clean_html, remove_links, non_ascii, lower, email_address, removeStopWords, punct, remove_\n",
    "import re\n",
    "\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# PREPROCESS THE DATA\n",
    "def preproc(df, colname):\n",
    "  df[colname] = df[colname].apply(func=clean_html)\n",
    "  df[colname] = df[colname].apply(func=remove_links)\n",
    "  df[colname] = df[colname].apply(func=non_ascii)\n",
    "  df[colname] = df[colname].apply(func=lower)\n",
    "  df[colname] = df[colname].apply(func=email_address)\n",
    "  # df[colname] = df[colname].apply(func=removeStopWords)\n",
    "  df[colname] = df[colname].apply(func=punct)\n",
    "  df[colname] = df[colname].apply(func=remove_)\n",
    "  return(df)\n",
    "\n",
    "df_clean = preproc(df, 'text')\n",
    "df_clean.drop('index', axis=1, inplace=True)\n",
    "df_clean['num_words'] = df_clean['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Convert 'labels' column to categorical data type\n",
    "df_clean['labels'] = pd.Categorical(df_clean['labels'])\n",
    "\n",
    "df_clean['labels'] = df_clean['labels'].cat.codes\n",
    "encoded_dict = {'legit':0, 'phish':1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean['labels'] = df_clean['labels'].apply(lambda x: 1-x)\n",
    "# df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your DataFrame is named 'df'\n",
    "# random_samples = df_clean.groupby('labels').apply(lambda x: x.sample(n=450, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your DataFrame is named 'df' and the label you want to drop from is '0'\n",
    "# label_to_drop = 1\n",
    "# num_samples_to_drop = 400\n",
    "\n",
    "# # Filter the DataFrame to select the label you want to drop from\n",
    "# label_df = random_samples[random_samples['labels'] == label_to_drop]\n",
    "\n",
    "# # Check if the number of samples to drop is greater than the available samples in the label_df\n",
    "# if num_samples_to_drop >= len(label_df):\n",
    "#     print(f\"Cannot drop {num_samples_to_drop} samples as there are only {len(label_df)} samples in the '{label_to_drop}' label.\")\n",
    "# else:\n",
    "#     # Randomly select the samples to drop\n",
    "#     samples_to_drop = label_df.sample(n=num_samples_to_drop, random_state=42)\n",
    "\n",
    "#     # Drop the selected samples from the original DataFrame\n",
    "#     random_samples = random_samples.drop(samples_to_drop.index)\n",
    "\n",
    "#     # Reset the indices of the DataFrame\n",
    "#     random_samples = random_samples.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    450\n",
       "1    450\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_samples.labels.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFBertModel \n",
    "\n",
    "df_train, df_test = train_test_split(df_clean, test_size=0.3, random_state=42,\n",
    "                                     stratify=df_clean['labels'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "max_len = 70\n",
    "\n",
    "X_train = tokenizer(\n",
    "    text=df_train['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=df_test['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VENUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "93/93 [==============================] - 1123s 12s/step - loss: 0.1303 - balanced_accuracy: 0.9517 - val_loss: 0.0547 - val_balanced_accuracy: 0.9866\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "# embeddings = dbert_model(input_ids, attention_mask = input_mask)[0]\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask = input_mask)[0] # 0 = last hidden state, 1 = poller_output\n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32, activation='relu')(out)\n",
    "\n",
    "y = Dense(2, activation='softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metric\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x = {'input_ids':X_train['input_ids'], 'attention_mask':X_train['attention_mask']},\n",
    "    y = to_categorical(df_train['labels']),\n",
    "    validation_data = ({'input_ids':X_test['input_ids'], 'attention_mask':X_test['attention_mask']},\n",
    "                        to_categorical(df_test['labels'])),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save(\"bert_mc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFBertModel layer\n",
    "bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\"bert_mc.h5\", custom_objects={\"TFBertModel\": bert})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 128s 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1119\n",
      "           1       0.96      0.92      0.94       149\n",
      "\n",
      "    accuracy                           0.99      1268\n",
      "   macro avg       0.98      0.96      0.97      1268\n",
      "weighted avg       0.99      0.99      0.99      1268\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'legit': 0, 'phish': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']})\n",
    "y_predicted = np.argmax(predicted, axis=1)\n",
    "print(classification_report(df_test['labels'], y_predicted,zero_division=0))\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 143ms/step\n",
      "Predicted label: 0\n",
      "True label: 0\n",
      "{'legit': 0, 'phish': 1}\n",
      "domain com discovery isn t preparing a bid for scripps networks although it s intrigued deadline\n",
      "hollywood has posted urgent news discovery isn t preparing a bid for scripps networks although it s\n",
      "intrigued for all of deadline s headlines follow us deadline on twitter this email was sent to by\n",
      "deadl\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Predicted label: 1\n",
      "True label: 1\n",
      "{'legit': 0, 'phish': 1}\n",
      "webmaster we have upgraded to 500mg email space login into your account to confirm if your account\n",
      "is still active then update the informations below for confirmation and upgrade full name email\n",
      "password confirm password note if you have not been upgraded please fill out the above to upgarde to\n",
      "500m\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "Predicted label: 0\n",
      "True label: 0\n",
      "{'legit': 0, 'phish': 1}\n",
      "a walk sorry to have been so out of touch and missing seeing you after your kind not a lot has been\n",
      "going on at home and work that have been very distracting i don t know if you are around this\n",
      "weekend but i d love to go for that walk if you have time michae\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "Predicted label: 0\n",
      "True label: 0\n",
      "{'legit': 0, 'phish': 1}\n",
      "re your dws story he called sean too but he didn t talk it s apparent he was reaching out to staff\n",
      "to get a sense of what she s like as a person and as a boss\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "Predicted label: 0\n",
      "True label: 0\n",
      "{'legit': 0, 'phish': 1}\n",
      "a rare pink and yellow diamond ring think pink large and natural fancy colored diamonds domain com\n",
      "310 774 6613 think pink this impressionable natural light pink radiant cut diamond weighs 2 53\n",
      "carats with vvs1 clarity and is certified by the gemological institute of america gia the rare\n",
      "center ston\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "# Select the index of the sample you want to predict\n",
    "for _ in range(5):\n",
    "    sample_index = random.randint(1, 1268)\n",
    "    # Get the input tensors for the selected sample\n",
    "    input_ids = X_test['input_ids'][sample_index]\n",
    "    attention_mask = X_test['attention_mask'][sample_index]\n",
    "\n",
    "    # Reshape the input tensors to match the expected shape\n",
    "    input_ids = np.reshape(input_ids, (1, -1))\n",
    "    attention_mask = np.reshape(attention_mask, (1, -1))\n",
    "\n",
    "    # Predict the sample\n",
    "    predicted = model.predict({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    y_predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "    # Get the true label for the selected sample\n",
    "    true_label = list(df_test['labels'])[sample_index]\n",
    "    email_text = list(df_test['text'])[sample_index]\n",
    "\n",
    "    # Print the prediction and true label\n",
    "    print(\"Predicted label:\", y_predicted[0])\n",
    "    print(\"True label:\", true_label)\n",
    "    print(encoded_dict)\n",
    "    # Set the desired width for each line\n",
    "    line_width = 100\n",
    "    formatted_string = textwrap.fill(email_text, line_width)\n",
    "    print(formatted_string[:300])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
